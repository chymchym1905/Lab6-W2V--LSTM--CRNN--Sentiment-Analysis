{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install pyvi"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kGnLGj-vwU9t"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd \n","import numpy as np\n","from string import digits\n","from collections import Counter\n","from pyvi import ViTokenizer\n","from gensim.models.word2vec import Word2Vec\n","from keras.utils.np_utils import to_categorical"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vKwxqnqHx3Na"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n"]}],"source":["print(tf.config.list_physical_devices())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Z7z2szHRwU9z","scrolled":true},"outputs":[],"source":["data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n","data_train.columns =['Class', 'Data']\n","data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n","data_test.columns =['Class', 'Data']"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1636625507608,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"D7U6TJVNwU90","outputId":"eedc3872-6e77-4919-ed23-8d66ad19d090"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5100, 2)\n","(1050, 2)\n","   Class  \\\n","0     -1   \n","1     -1   \n","2     -1   \n","3     -1   \n","4     -1   \n","\n","                                                                                                  Data  \n","0  Mình đã dùng anywhere thế hệ đầu, quả là đầy thất vọng, hiện tại đang vứt xó. Giá thì đắt, ngốn ...  \n","1  Quan tâm nhất là độ trễ có cao không, dùng thi thoảng nó cứ trễ bực mình, đấy mới chỉ là dùng vă...  \n","2                     dag xài con cùi bắp 98k....pin trâu, mỗi tội đánh liên minh ức chế đập hết 2 con  \n","3  logitech chắc hàng phải tiền triệu trở lên dùng mới thích chứ em dùng con có 400k thấy được vài ...  \n","4  Đang xài con m175 cùi mía , nhà xài nhiều chuột nên thử con này con kia chơi và kết quả là sau 3...  \n"]}],"source":["print(data_train.shape)\n","print(data_test.shape)\n","pd.set_option('max_colwidth', 100)\n","print(data_train.head())"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"prV4XxvywU90"},"outputs":[],"source":["labels = data_train.iloc[:, 0].values\n","reviews = data_train.iloc[:, 1].values"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hQZymxX0wU91"},"outputs":[],"source":["encoded_labels = []\n","\n","for label in labels:\n","    if label == -1:\n","        encoded_labels.append([1,0,0])\n","    elif label == 0:\n","        encoded_labels.append([0,1,0])\n","    else:\n","        encoded_labels.append([0,0,1])\n","\n","encoded_labels = np.array(encoded_labels)  "]},{"cell_type":"code","execution_count":18,"metadata":{"id":"34J9F70iwU91"},"outputs":[],"source":["reviews_processed = []\n","unlabeled_processed = [] \n","for review in reviews:\n","    review_cool_one = ''.join([char for char in review if char not in digits])\n","    reviews_processed.append(review_cool_one)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"-uevU-I5wU92"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews = []\n","all_words = []\n","for review in reviews_processed:\n","    review = ViTokenizer.tokenize(review.lower())\n","    word_reviews.append(review.split())\n","   "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"VvZBk6kfwU93"},"outputs":[],"source":["EMBEDDING_DIM = 400 # how big is each word vector\n","MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n","MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"xcAfN337wU93"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"0rXI-fxLwU94"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews)\n","word_index = tokenizer.word_index\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"6wAXC5HrwU94"},"outputs":[],"source":["data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = encoded_labels"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1636625513548,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"9n7P5xlZwU94","outputId":"593bc5b3-12b5-4013-ab9f-cca7ce907602"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (5100, 300)\n","Shape of label train and validation tensor: (5100, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data.shape)\n","print('Shape of label train and validation tensor:', labels.shape)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8223,"status":"ok","timestamp":1636625591392,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"cFwY64D7wU95","outputId":"92a05467-7a63-43ae-9b08-165e71364523"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab size 7919\n"]}],"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","word_vectors = KeyedVectors.load_word2vec_format('vi-model-CBOW.bin', binary=True)\n","\n","\n","vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n","embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","print(\"vocab size\", vocabulary_size)\n","for word, i in word_index.items():\n","    if i>=MAX_VOCAB_SIZE:\n","        continue\n","    try:\n","        embedding_vector = word_vectors[word]\n","        embedding_matrix[i] = embedding_vector\n","    except KeyError:\n","        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","del(word_vectors)\n","\n","from keras.layers import Embedding\n","embedding_layer = Embedding(vocabulary_size,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            trainable=True)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1636625591393,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"TF5jaU0-wU95","outputId":"9a57539e-768c-4f5b-ebca-4a51c6d517db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 300)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 300, 400)             3167600   ['input_3[0][0]']             \n","                                                                                                  \n"," reshape_2 (Reshape)         (None, 300, 400)             0         ['embedding[2][0]']           \n","                                                                                                  \n"," conv1d_6 (Conv1D)           (None, 300, 100)             120100    ['reshape_2[0][0]']           \n","                                                                                                  \n"," conv1d_7 (Conv1D)           (None, 300, 100)             160100    ['reshape_2[0][0]']           \n","                                                                                                  \n"," conv1d_8 (Conv1D)           (None, 300, 100)             200100    ['reshape_2[0][0]']           \n","                                                                                                  \n"," max_pooling1d_6 (MaxPoolin  (None, 1, 100)               0         ['conv1d_6[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," max_pooling1d_7 (MaxPoolin  (None, 1, 100)               0         ['conv1d_7[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," max_pooling1d_8 (MaxPoolin  (None, 1, 100)               0         ['conv1d_8[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," concatenate_2 (Concatenate  (None, 1, 300)               0         ['max_pooling1d_6[0][0]',     \n"," )                                                                   'max_pooling1d_7[0][0]',     \n","                                                                     'max_pooling1d_8[0][0]']     \n","                                                                                                  \n"," flatten_2 (Flatten)         (None, 300)                  0         ['concatenate_2[0][0]']       \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 300)                  0         ['flatten_2[0][0]']           \n","                                                                                                  \n"," dense_2 (Dense)             (None, 3)                    903       ['dropout_2[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 3648803 (13.92 MB)\n","Trainable params: 3648803 (13.92 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["from keras.models import Model\n","from keras.layers import Input, Reshape, Conv1D, MaxPool1D, concatenate, Flatten, Dropout, Dense\n","from keras.callbacks import EarlyStopping\n","from keras.models import Model\n","from keras import regularizers\n","sequence_length = data.shape[1]\n","filter_sizes = [3,4,5]\n","num_filters = 100\n","drop = 0.5\n","\n","inputs = Input(shape=(sequence_length,))\n","embedding = embedding_layer(inputs)\n","\n","################## LSTM ONLY ###############################\n","# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","################# SINGLE LSTM ####################\n","# lstm_0 = LSTM(512)(reshape)\n","\n","# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","# lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n","# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","# lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","\n","################## CRNN ####################################\n","reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","\n","conv_0 = MaxPool1D(300)(conv_0)\n","conv_1 = MaxPool1D(300)(conv_1)\n","conv_2 = MaxPool1D(300)(conv_2)\n","# Reshape output to match RNN dimension\n","# conv_0 = Reshape((-1, num_filters))(conv_0)\n","# conv_1 = Reshape((-1, num_filters))(conv_1)\n","# conv_2 = Reshape((-1, num_filters))(conv_2)\n","\n","concat = concatenate([conv_0, conv_1, conv_2])\n","concat = Flatten()(concat)\n","# lstm_0 = LSTM(512)(concat)\n","\n","# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n","# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","# lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","dropout = Dropout(drop)(concat)\n","output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n","\n","# this creates a model that includes\n","model = Model(inputs, output)\n","\n","\n","adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUZo87gZGYlz"},"outputs":[],"source":["### IF YOU HAVE MODEL WEIGHT AND WANNA LOAD IT\n","model.load_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n"]}],"source":["tf.debugging.set_log_device_placement(True)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices()))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83275,"status":"ok","timestamp":1636625724079,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"HTJGHh2hwU96","outputId":"d7ad4ba3-e4a5-41b5-8717-2f6b851accfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"," 4/16 [======>.......................] - ETA: 37s - loss: 8.6711 - accuracy: 0.3721"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mi:\\Bách Khoa\\CO3085\\lab\\Lab6-W2V+[LSTM, CRNN] Sentiment Analysis\\word2vec+[crnn, cnn,rnn].ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3085/lab/Lab6-W2V%2B%5BLSTM%2C%20CRNN%5D%20Sentiment%20Analysis/word2vec%2B%5Bcrnn%2C%20cnn%2Crnn%5D.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3085/lab/Lab6-W2V%2B%5BLSTM%2C%20CRNN%5D%20Sentiment%20Analysis/word2vec%2B%5Bcrnn%2C%20cnn%2Crnn%5D.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m callbacks_list \u001b[39m=\u001b[39m [early_stopping]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3085/lab/Lab6-W2V%2B%5BLSTM%2C%20CRNN%5D%20Sentiment%20Analysis/word2vec%2B%5Bcrnn%2C%20cnn%2Crnn%5D.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data, labels, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3085/lab/Lab6-W2V%2B%5BLSTM%2C%20CRNN%5D%20Sentiment%20Analysis/word2vec%2B%5Bcrnn%2C%20cnn%2Crnn%5D.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks_list, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n","File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]\n","\n","model.fit(data, labels, validation_split=0.2,\n","          epochs=10, batch_size=256, callbacks=callbacks_list, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDWU9oCBwU96"},"outputs":[],"source":["labels_test = data_test.iloc[:, 0].values\n","reviews_test = data_test.iloc[:, 1].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2zBNnGJwU96"},"outputs":[],"source":["encoded_labels_test = []\n","\n","for label_test in labels_test:\n","    if label_test == -1:\n","        encoded_labels_test.append([1,0,0])\n","    elif label_test == 0:\n","        encoded_labels_test.append([0,1,0])\n","    else:\n","        encoded_labels_test.append([0,0,1])\n","\n","encoded_labels_test = np.array(encoded_labels_test)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArJFkP6fwU97"},"outputs":[],"source":["reviews_processed_test = []\n","unlabeled_processed_test = [] \n","for review_test in reviews_test:\n","    review_cool_one = ''.join([char for char in review_test if char not in digits])\n","    reviews_processed_test.append(review_cool_one)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV9bR1nlwU97"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews_test = []\n","all_words = []\n","for review_test in reviews_processed_test:\n","    review_test = ViTokenizer.tokenize(review_test.lower())\n","    word_reviews_test.append(review_test.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvf83C-QwU98"},"outputs":[],"source":["sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n","labels_test = encoded_labels_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1636625740257,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"6Zubh3jfwU98","outputId":"de6997b1-8ce4-4430-ad96-bed024518e38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (1050, 300)\n","Shape of label train and validation tensor: (1050, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data_test.shape)\n","print('Shape of label train and validation tensor:', labels_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":855,"status":"ok","timestamp":1636625742613,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"uzDo7cKqwU98","outputId":"9b01dd3f-f982-41f6-d3b6-61e59120a4b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 1s 22ms/step - loss: 2.3534 - accuracy: 0.6133\n"]}],"source":["score = model.evaluate(data_test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1636625745980,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"cE6zEWGNwU99","outputId":"08bef793-78ce-48b5-80a6-45721ba4d79e"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 2.35\n","accuracy: 61.33%\n"]}],"source":["print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1636625423692,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"-6K00ZjfF-9M","outputId":"a64e89a6-8368-4440-f52c-c337bd98902b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_SlhL45wU99"},"outputs":[],"source":["model.save_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytgukqXFGruD"},"outputs":[],"source":["!cp model.h5 /content/drive/MyDrive"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"word2vec+[crnn, cnn,rnn].ipynb","provenance":[{"file_id":"1srU5584QF_d3NCE9UX9q-6ZgxUTo6FBt","timestamp":1633867144346},{"file_id":"1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ","timestamp":1616587562665}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
